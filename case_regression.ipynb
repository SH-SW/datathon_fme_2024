{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration and Data Loading\n",
    "\n",
    "Loading the California Housing dataset and splitting it into features and target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Splitting data into features (X) and target (y)\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Defining parameters\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Pipeline\n",
    "\n",
    "Setting up a preprocessing pipeline with transformations for numerical and categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying numerical and categorical features\n",
    "numeric_features = list(X_train.select_dtypes(include=[np.number]).columns)\n",
    "categorical_features = list(X_train.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# Defining transformations for numerical features\n",
    "numeric_transformers = [('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=2, include_bias=False))]\n",
    "\n",
    "# Defining transformations for categorical features\n",
    "categorical_transformers = [('onehot', OneHotEncoder(handle_unknown='ignore'))]\n",
    "\n",
    "# Building preprocessor with transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=numeric_transformers), numeric_features),\n",
    "        ('cat', Pipeline(steps=categorical_transformers), categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "Selecting and configuring the model based on the configuration. Here we choose Random Forest Regressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model parameters\n",
    "model_type = 'random_forest'\n",
    "n_estimators = 100\n",
    "max_depth = 10\n",
    "min_samples_split = 2\n",
    "\n",
    "# Choosing and initialize the model\n",
    "if model_type == 'linear_regression':\n",
    "    model = LinearRegression()\n",
    "elif model_type == 'random_forest':\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=random_state\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Model '{model_type}' not supported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Pipeline\n",
    "\n",
    "Combining the preprocessor and the model into a complete pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline with preprocessing and model steps\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Logging in MLflow\n",
    "\n",
    "Trainning the model and log parameters, metrics, and the model itself in MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting an MLflow run\n",
    "mlflow.start_run()\n",
    "\n",
    "# Logging parameters to MLflow\n",
    "mlflow.log_param('model_type', model_type)\n",
    "mlflow.log_param('n_estimators', n_estimators)\n",
    "mlflow.log_param('max_depth', max_depth)\n",
    "mlflow.log_param('test_size', test_size)\n",
    "mlflow.log_param('min_samples_split', min_samples_split)\n",
    "\n",
    "# Fitting the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Logging the model to MLflow\n",
    "mlflow.sklearn.log_model(pipeline, artifact_path='model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluating the model on the test set, calculate metrics, and log them in MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Logging metrics to MLflow\n",
    "mlflow.log_metric('mse', mse)\n",
    "mlflow.log_metric('mae', mae)\n",
    "mlflow.log_metric('r2_score', r2)\n",
    "\n",
    "# Printing metrics\n",
    "print(f'MSE: {mse}, MAE: {mae}, R2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Logging\n",
    "\n",
    "Logging feature importances (for models that support it) in MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting feature importances from the model (if available)\n",
    "model = pipeline.named_steps['regressor']\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Checking if the model has feature_importances_ or coefficients\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    importances = model.feature_importances_\n",
    "elif hasattr(model, 'coef_'):\n",
    "    importances = np.abs(model.coef_)\n",
    "else:\n",
    "    importances = None\n",
    "    print(\"The model does not have 'feature_importances_' or 'coef_' attributes\")\n",
    "\n",
    "if importances is not None:\n",
    "    # Creating a DataFrame for feature importances\n",
    "    feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Saving and logging feature importances as CSV\n",
    "    feat_imp_csv = 'feature_importance.csv'\n",
    "    feat_imp_df.to_csv(feat_imp_csv, index=False)\n",
    "    mlflow.log_artifact(feat_imp_csv)\n",
    "\n",
    "    # Plotting and saving feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Saving and log the plot\n",
    "    feat_imp_png = 'feature_importance.png'\n",
    "    plt.savefig(feat_imp_png)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(feat_imp_png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Values Logging\n",
    "\n",
    "Calculating and log SHAP values to explain the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for SHAP images\n",
    "shap_images_dir = \"shap_images\"\n",
    "os.makedirs(shap_images_dir, exist_ok=True)\n",
    "\n",
    "# Using a sample from the test data to speed up SHAP computation\n",
    "X_test_sample = X_test.sample(n=100, random_state=random_state)\n",
    "\n",
    "# Transforming the data with preprocessing\n",
    "X_transformed = pipeline.named_steps['preprocessor'].transform(X_test_sample)\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "# Selecting SHAP explainer based on model type\n",
    "if model_type == 'random_forest':\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "else:\n",
    "    explainer = shap.LinearExplainer(model, X_transformed)\n",
    "\n",
    "# Calculating SHAP values\n",
    "shap_values = explainer.shap_values(X_transformed)\n",
    "\n",
    "# Ploting SHAP summary and save\n",
    "shap_summary_png = os.path.join(shap_images_dir, 'shap_summary_plot.png')\n",
    "shap.summary_plot(shap_values, features=X_transformed_df, feature_names=feature_names, show=False)\n",
    "plt.savefig(shap_summary_png, bbox_inches='tight')\n",
    "plt.close()\n",
    "mlflow.log_artifact(shap_summary_png, artifact_path='shap_plots')\n",
    "\n",
    "# Ploting SHAP dependence for the most important feature and save\n",
    "top_feature_index = np.argmax(np.abs(shap_values).mean(0))\n",
    "feature_name = feature_names[top_feature_index]\n",
    "shap_dependence_png = os.path.join(shap_images_dir, 'shap_dependence_plot.png')\n",
    "shap.dependence_plot(feature_name, shap_values, X_transformed_df, feature_names=feature_names, show=False)\n",
    "plt.savefig(shap_dependence_png, bbox_inches='tight')\n",
    "plt.close()\n",
    "mlflow.log_artifact(shap_dependence_png, artifact_path='shap_plots')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End the MLflow Experiment\n",
    "\n",
    "Ending the MLflow run to finalize the logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ending the MLflow run\n",
    "mlflow.end_run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
